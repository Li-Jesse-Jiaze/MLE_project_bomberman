# Roadmap

## 背景：增强学习（RL）

通过试错在给定世界的状态表示下训练智能体来做出决策，目的是最大化累积奖励。

### Q-Leaning

无模型的方法

学习动作价值函数$Q(s, a)$，其中$s$为状态，$a$为动作，最大化长期的预期累积奖励

通过以下方程迭代更新Q值：

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$

$s_t$：当前状态，$a_t$：选择的动作，$r_t$：收到的即时奖励，$\gamma$：折扣因子，$\alpha$：学习率。

### DQN

1. **函数近似**：使用神经网络来近似Q值

2. **经验回放**：在回放缓冲区中存储经验（状态、动作、奖励、下一个状态）并在训练期间抽取小批量样本

3. **目标网络**：使用两个网络：目标网络和策略网络。目标网络落后于策略网络，其Q值被用作训练的目标，有助于减少训练期间Q值估计的方差。

## 其他方法

Proximal Policy Optimization (PPO)

## 需要做的

**存储经验**：设计数据结构记录之前的状态和动作之类

> 我感觉用队列比较合适
>
> 这里难点主要是如何有效的组织训练

**设计特征**：从半结构化数据`state_dict`转化为可以输入到神经网络的特征

> 我目前的想法是用一个以智能体自身为中心的二维数组记录整个地图，用不同的数值表示敌人、墙、金币之类的，这样方便直接输入到CNN
>
> 但这样一些状态比如当前轮数、敌人分数之类的就利用不上

**模型结构**：不能太复杂，需要满足老头的设备

> 网络相当于一个很简单的分类器，输入特征，输出六个动作之一

**损失函数和优化器**

> **MSEloss** Cross-Entropy Loss HuberLoss SmoothL1Loss 都可以感觉
>
> RMSprop AdamW 二选一